{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNlgzqPNMuRh"
      },
      "source": [
        "#Aprendizado por Reforço e o Dilema do Prisioneiro\n",
        "Neste estudo contaremos com a ajuda da biblioteca Axelrod (citar artigo) para simular rodadas do jogo Dilema do Prisioneiro. O jogo inventado por"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hjWtMmAQuq4"
      },
      "source": [
        "# Referências\n",
        "\n",
        "Axelrod:\n",
        "\n",
        "                  https://hash.ai/blog/improving-the-prisoners-dilemma-with-q-learning\n",
        "                  https://github.com/Axelrod-Python/Axelrod\n",
        "                  https://axelrod.readthedocs.io/en/fix-documentation/tutorials/index.html\n",
        "\n",
        "\n",
        "Game Theory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjgX1z87rZlP",
        "outputId": "59963abc-6b9e-4f46-8b72-49f23f575f5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting axelrod\n",
            "  Downloading Axelrod-4.13.0-py2.py3-none-any.whl (348 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.4/348.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from axelrod) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from axelrod) (2023.6.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from axelrod) (0.12.0)\n",
            "Requirement already satisfied: dask>=2.9.2 in /usr/local/lib/python3.10/dist-packages (from axelrod) (2023.8.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.10/dist-packages (from axelrod) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.17.4 in /usr/local/lib/python3.10/dist-packages (from axelrod) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from axelrod) (1.5.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from axelrod) (6.0.1)\n",
            "Requirement already satisfied: scipy>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from axelrod) (1.10.1)\n",
            "Requirement already satisfied: tqdm>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from axelrod) (4.66.1)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.9.2->axelrod) (8.1.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.9.2->axelrod) (23.1)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.9.2->axelrod) (1.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.9.2->axelrod) (6.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.3->axelrod) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.3->axelrod) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.3->axelrod) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.3->axelrod) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.3->axelrod) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.3->axelrod) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.3->axelrod) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->axelrod) (2023.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask>=2.9.2->axelrod) (3.16.2)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.2.0->dask>=2.9.2->axelrod) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.3->axelrod) (1.16.0)\n",
            "Installing collected packages: axelrod\n",
            "Successfully installed axelrod-4.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install axelrod"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIUDT-pz_NFA",
        "outputId": "6beded58-aed0-47b0-ce59-fc155672cfbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import axelrod as axl\n",
        "from matplotlib import colors\n",
        "from matplotlib.ticker import PercentFormatter\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "The payoff is that in case the initiator agent cooperate they are rewarded\n",
        "         2 and 0 points, if the participant, either cooperates back or betrays\n",
        "On the other hand, if the initiator decides to betray it will be rewarded\n",
        "         3 and 1 points, depending on the participant's actions\n",
        "\n",
        "         Matrix = CC CD\n",
        "                  DC DD\n",
        "\n",
        "                  C = Cooperates, D = Defects\n",
        "                  CD = Initiator cooperated and participant defected\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "print(' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "beBPl2KSdekF"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "from typing import Dict, Union\n",
        "\n",
        "from axelrod.action import Action, actions_to_str\n",
        "from axelrod.player import Player\n",
        "\n",
        "Score = Union[int, float]\n",
        "\n",
        "C, D = Action.C, Action.D\n",
        "\n",
        "class MyQLearner(Player):\n",
        "    name = \"My QLearner\"\n",
        "    classifier = {\n",
        "        \"memory_depth\": float(\"inf\"),  # Long memory\n",
        "        \"stochastic\": True,\n",
        "        \"long_run_time\": False,\n",
        "        \"inspects_source\": False,\n",
        "        \"manipulates_source\": False,\n",
        "        \"manipulates_state\": False,\n",
        "    }\n",
        "    learning_rate = 0.9\n",
        "    discount_rate = 0.1\n",
        "    action_selection_parameter = 0.15\n",
        "    memory_length = 10\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"Initialises the player by picking a random strategy.\"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Set this explicitly, since the constructor of super will not pick it up\n",
        "        # for any subclasses that do not override methods using random calls.\n",
        "        self.classifier[\"stochastic\"] = True\n",
        "\n",
        "        self.prev_action = None  # type: Action\n",
        "        self.original_prev_action = None  # type: Action\n",
        "        self.score = 0\n",
        "        self.Qs = OrderedDict({\"\": OrderedDict(zip([C, D], [5, 5]))})\n",
        "        self.Vs = OrderedDict({\"\": 5})\n",
        "        self.prev_state = \"\"\n",
        "\n",
        "    def receive_match_attributes(self):\n",
        "        (R, P, S, T) = self.match_attributes[\"game\"].RPST()\n",
        "        self.payoff_matrix = {C: {C: R, D: S}, D: {C: T, D: P}}\n",
        "\n",
        "    def strategy(self, opponent: Player) -> Action:\n",
        "        \"\"\"Runs a qlearn algorithm while the tournament is running.\"\"\"\n",
        "        if len(self.history) == 0:\n",
        "            self.prev_action = self._random.random_choice()\n",
        "            self.original_prev_action = self.prev_action\n",
        "        state = self.find_state(opponent)\n",
        "        reward = self.find_reward(opponent)\n",
        "        if state not in self.Qs:\n",
        "            self.Qs[state] = OrderedDict(zip([C, D], [5, 5]))\n",
        "            self.Vs[state] = 5\n",
        "        self.perform_q_learning(self.prev_state, state, self.prev_action, reward)\n",
        "        action = self.select_action(state)\n",
        "        self.prev_state = state\n",
        "        self.prev_action = action\n",
        "        return action\n",
        "\n",
        "    def select_action(self, state: str) -> Action:\n",
        "        \"\"\"\n",
        "        Selects the action based on the epsilon-soft policy\n",
        "        \"\"\"\n",
        "        rnd_num = self._random.random()\n",
        "        p = 1.0 - self.action_selection_parameter\n",
        "\n",
        "        if len(self.history)%5 == 0 and len(self.history) > 0:\n",
        "          self.action_selection_parameter -= 0.015\n",
        "\n",
        "        if rnd_num < p:\n",
        "            return max(self.Qs[state], key=lambda x: self.Qs[state][x])\n",
        "        return self._random.random_choice()\n",
        "\n",
        "    def find_state(self, opponent: Player) -> str:\n",
        "        \"\"\"\n",
        "        Finds the my_state (the opponents last n moves +\n",
        "        its previous proportion of playing C) as a hashable state\n",
        "        \"\"\"\n",
        "        if len(opponent.history) == 0:\n",
        "          prob = \"0.0\"\n",
        "        else:\n",
        "          prob = \"{:.1f}\".format(opponent.cooperations/len(opponent.history))\n",
        "\n",
        "        action_str = actions_to_str(opponent.history[-self.memory_length:])\n",
        "\n",
        "        return action_str + prob\n",
        "\n",
        "    def perform_q_learning(self, prev_state: str, state: str, action: Action, reward):\n",
        "        \"\"\"\n",
        "        Performs the qlearning algorithm\n",
        "        \"\"\"\n",
        "        self.Qs[prev_state][action] = (1.0 - self.learning_rate) * self.Qs[prev_state][\n",
        "            action\n",
        "        ] + self.learning_rate * (reward + self.discount_rate * self.Vs[state])\n",
        "        self.Vs[prev_state] = max(self.Qs[prev_state].values())\n",
        "\n",
        "    def find_reward(self, opponent: Player) -> Dict[Action, Dict[Action, Score]]:\n",
        "        \"\"\"\n",
        "        Finds the reward gained on the last iteration\n",
        "        \"\"\"\n",
        "\n",
        "        if len(opponent.history) == 0:\n",
        "            opp_prev_action = self._random.random_choice()\n",
        "        else:\n",
        "            opp_prev_action = opponent.history[-1]\n",
        "        return self.payoff_matrix[self.prev_action][opp_prev_action]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmfaK0BtnPJo",
        "outputId": "1e92d226-3fd0-4cee-8a15-0bc1ca5d3e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "My QLearner 16776\n",
            "Arrogant QLearner 11129\n",
            "Hesitant QLearner 11155\n",
            "Cautious QLearner 11221\n",
            "Risky QLearner 11179\n",
            "Grudger 9075\n"
          ]
        }
      ],
      "source": [
        "players = [MyQLearner(), axl.ArrogantQLearner(),\n",
        "           axl.HesitantQLearner(), axl.CautiousQLearner(),\n",
        "           axl.RiskyQLearner(), axl.Grudger()]\n",
        "\n",
        "tournament = axl.Tournament(\n",
        "      players=players,\n",
        "      turns=200,\n",
        "      repetitions=5)\n",
        "results = tournament.play(progress_bar=False)\n",
        "print('\\n')\n",
        "\n",
        "#for name in results.ranked_names:\n",
        "     #print(name)\n",
        "\n",
        "\n",
        "for i in range(len(results.players)):\n",
        "  print(results.players[i], np.sum(results.scores[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9r_sDT3nrX1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b3311e9-61d0-4511-bc4a-bc4c263bab0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coop:  [139.182 140.496]\n",
            "Placar médio:  [524.222 517.652] \n",
            "\n",
            "Coop:  [161.388 161.54 ]\n",
            "Placar médio:  [542.532 541.772] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "competitors = [MyQLearner(), axl.TitForTat(), axl.Cooperator(), axl.Defector()]\n",
        "n = len(competitors)\n",
        "total_scores = np.zeros((n, 2))\n",
        "total_coop = np.zeros((n, 2))\n",
        "\n",
        "for x in range(n):\n",
        "  sum = 0\n",
        "  sum2 = 0\n",
        "  cooperation = np.array([0, 0])\n",
        "  scores = np.array([0, 0])\n",
        "  for _ in range(500):\n",
        "    players = [MyQLearner(), competitors[x]]\n",
        "\n",
        "    match2 = axl.Match(players, turns=200, match_attributes={\"length\": float('inf')})\n",
        "\n",
        "    resultados = match2.play()\n",
        "\n",
        "    scores[0] += match2.final_score()[0]\n",
        "    scores[1] += match2.final_score()[1]\n",
        "\n",
        "    cooperation[0] += match2.cooperation()[0]\n",
        "    cooperation[1] += match2.cooperation()[1]\n",
        "\n",
        "  total_coop[x] = cooperation/500\n",
        "  total_scores[x] = scores/500\n",
        "  print(\"Coop: \", cooperation/500)\n",
        "  print(\"Placar médio: \", scores/500, \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JIyrgZgWGN0"
      },
      "outputs": [],
      "source": [
        "strats = [x.name for x in competitors]\n",
        "dict_scores = {\"Pontuação\": list(total_scores[:, 0])}\n",
        "dict_scores[\"Oponente\"] = list(total_scores[:, 1])\n",
        "\n",
        "x = np.arange(len(strats))  # the label locations\n",
        "width = 0.25  # the width of the bars\n",
        "multiplier = 0\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (12, 6), layout='constrained')\n",
        "\n",
        "for attribute, measurement in dict_scores.items():\n",
        "    offset = width * multiplier*1.2\n",
        "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
        "    ax.bar_label(rects, padding=3)\n",
        "    multiplier += 1\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel('Pontuação média')\n",
        "ax.set_title('Desempenho individual da estratégia ' + strats[0])\n",
        "ax.set_xticks(x + width, strats)\n",
        "ax.legend(loc='upper left', ncols=3)\n",
        "ax.set_ylim(0, 1100)\n",
        "plt.savefig(\"desempenho_\"+ strats[0] +\".png\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strats = [x.name for x in competitors]\n",
        "dict_scores = {\"Pontuação\": list(total_coop[:, 0])}\n",
        "dict_scores[\"Oponente\"] = list(total_coop[:, 1])\n",
        "\n",
        "x = np.arange(len(strats))  # the label locations\n",
        "width = 0.25  # the width of the bars\n",
        "multiplier = 0\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (12, 6), layout='constrained')\n",
        "\n",
        "for attribute, measurement in dict_scores.items():\n",
        "    offset = width * multiplier*1.2\n",
        "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
        "    ax.bar_label(rects, padding=3)\n",
        "    multiplier += 1\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel('Cooperação média' )\n",
        "ax.set_title('Desempenho individual da estratégia '+ strats[0])\n",
        "ax.set_xticks(x + width, strats)\n",
        "ax.legend(loc='upper left', ncols=3)\n",
        "ax.set_ylim(0, 250)\n",
        "plt.savefig(\"coop_\"+ strats[0] +\".png\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NUeZW5T4vLTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFiXAwTzp2bC"
      },
      "outputs": [],
      "source": [
        "sum = 0\n",
        "n_participants = 10\n",
        "sum2 = np.zeros(n_participants)\n",
        "\n",
        "for j in range(25):\n",
        "  tournament_participants = [s() for s in axl.axelrod_first_strategies[:n_participants-5]]\n",
        "  tournament_participants += [MyQLearner(), axl.ArrogantQLearner(),\n",
        "           axl.HesitantQLearner(), axl.CautiousQLearner(),\n",
        "           axl.RiskyQLearner()]\n",
        "\n",
        "  tournament = axl.Tournament(\n",
        "      players=tournament_participants,\n",
        "      turns=200,\n",
        "      repetitions=5,\n",
        "      noise=0.0\n",
        "      )\n",
        "\n",
        "  results = tournament.play(progress_bar=False)\n",
        "\n",
        "  if \"My QLearner\" == results.ranked_names[0]:\n",
        "    sum += 1\n",
        "  for i in range(len(results.players)):\n",
        "    sum2[i] += np.sum(results.scores[i])\n",
        "\n",
        "print(\"Frequencia de pódio: \", sum)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NKubJgAZ3mW"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 8))\n",
        "\n",
        "names = [s.split(\":\")[0] for s in results.ranked_names[::-1]]\n",
        "names = [s.split(\" by \")[-1] for s in names]\n",
        "#names = results.ranked_names[::-1]\n",
        "counts = sum2/25\n",
        "counts = sorted(counts)\n",
        "\n",
        "colors = [\"#1984c5\", \"#22a7f0\", \"#63bff0\", \"#e1a692\", \"#de6e56\", \"#e14b31\", \"#c23728\"]\n",
        "# \"#a7d5ed\", \"#e2e2e2\",\n",
        "axis = ax.bar(names, counts, label=names, color=colors)\n",
        "ax.bar_label(axis, padding=2)\n",
        "\n",
        "plt.ylim([np.min(counts)-100, np.max(counts)+150])\n",
        "ax.set_ylabel('Pontuação média em jogos de 200 rodadas e 5 repetições')\n",
        "ax.set_title('Estratégias fixas X Agente de Q-Learning')\n",
        "ax.legend()\n",
        "plt.savefig(\"tournament6.png\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifNn8UMdohvj"
      },
      "outputs": [],
      "source": [
        "for i in range(len(results.players)):\n",
        "  print(results.players[i], results.scores[i], results.wins[i])\n",
        "\n",
        "# random decai 15% em 1.5%5\n",
        "# init 5\n",
        "# learning rate 0.5\n",
        "#"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}